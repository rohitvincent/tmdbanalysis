{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Movies from 2000 - 2019 using the TMDB Database \n",
    "<b>Author:</b> Rohit Vincent <br>\n",
    "<b>Source :</b> https://www.themoviedb.org/ <br>\n",
    "<b>Description of the database:</b> The Movie Database (TMDb) is a community built movie and TV database. Every piece of data has been added by our amazing community dating back to 2008. TMDb's strong international focus and breadth of data is largely unmatched and something we're incredibly proud of. Put simply, we live and breathe community and that's precisely what makes us different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Following javascript code is to disable the scroll function for the output of the notebook</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Install wordcloud if not already installed</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Ensuring storage of plots within notebook</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests  # to make TMDB API calls\n",
    "import seaborn as sns\n",
    "from nltk import word_tokenize as tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define awesome function to print better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function to display colors & text styles\n",
    "def printmd(string, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Global Variables/Parameters\n",
    "<ul>\n",
    "    <li><b>REQUEST_LIMIT</b>: TMDB API has request limit of 40 requests per 10 seconds. \n",
    "To ensure the program doesn't overload the number of requests, we will limit the number of requests per 10 seconds based on the global parameter set below. We will set this to 39 currently.</li>\n",
    "    <li><b>FILENAME</b>: Filename to where data fetched from API should be saved & loaded for further processing.</li>\n",
    "    <li><b>APIKEY</b>: API key value for user to access the TMDB API</li>\n",
    "</ul>\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request Limit for TMDB API per 10 seconds\n",
    "REQUEST_LIMIT = 39\n",
    "# Filename to store/load data from API\n",
    "FILENAME='tmdb_dump.csv'\n",
    "# API key\n",
    "APIKEY = 'b67e8d052c594195b61e2533a4968dd7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare & Set Other Variables/ Utility Functions\n",
    "\n",
    "<b>Declare PorterStemmer for stemming reviews of movies</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Set Figure Size of Plots</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,7)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define Functions to plot Bar Graphs & Wordclouds</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Generate Random Colours\n",
    "def getColors(length):\n",
    "    colors = list()\n",
    "    for i in range(length):\n",
    "        colors.append(list(np.random.rand(3,)))\n",
    "    return colors\n",
    "\n",
    "# Plot bar graph\n",
    "def plotbar(common_words,title,xlabel,ylabel):\n",
    "    # Unzip to get labels, values\n",
    "    labels, ys = zip(*common_words)\n",
    "    xs = np.arange(len(labels)) \n",
    "    # Set title\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.bar(xs, ys,align='center', color=getColors(len(xs)))\n",
    "    plt.xticks(xs, labels) #Replace default x-ticks with xs, then replace xs with labels\n",
    "    plt.yticks(ys)\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Generate wordcloud\n",
    "def disp_wordcloud(text,title):\n",
    "    #Remove stop words\n",
    "    wordcloud = WordCloud(stopwords=stopwords.words('english'), background_color=\"white\").generate(' '.join(text))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define functions to clean text using the following steps:</b>\n",
    "1. Calculate Lexical Diversity of text\n",
    "2. Clean Text\n",
    "<ul>\n",
    "    <li> Remove punctuations </li>\n",
    "    <li> Convert word to lowercase </li>\n",
    "    <li> Remove stop words such as a, an, the </li>\n",
    "    <li> Convert to root form of word using PorterStemmer </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Diversity     \n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "    \n",
    "# Perform all the above + stemming        \n",
    "def cleanText(text):     \n",
    "    #Remove Punctuations, stopwords, convert to lower case & stem the words\n",
    "    lowercase_text = [word.lower() for word in text if word.isalnum()]\n",
    "    # Remove stop words\n",
    "    filtered_words = [word.lower() for word in lowercase_text if word.lower() not in stopwords.words('english')]\n",
    "    # Stem\n",
    "    stemmed_words = [porter.stem(word) for word in filtered_words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define Class tmdb for analysis of the dataset</b>\n",
    "\n",
    "Following parameters are defined in the class\n",
    "<ul>\n",
    "    <li><b> Limit: </b> Limit of requests</li>\n",
    "    <li><b> Pagelimit: </b> Limit of pages for a request. TMDB has max of 1000 which is defaulted here</li>\n",
    "    <li><b> API Key: </b> API Key for TMDB</li>\n",
    "    <li><b> Start: </b> Start Year of Data Selection & Analysis</li>\n",
    "    <li><b> End: </b> End Year of Data Selection & Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following functions are defined in the class:\n",
    "<ul>\n",
    "    <li> <b>check_requestLimit : </b>Function to check if request has reached limit in the last 10 seconds.</li>\n",
    "    <li> <b>fetchFilmDetails : </b>Fetch data set from API & Save to CSV.</li>\n",
    "    <li> <b>processDetails : </b>Function to Preprocess dataset & apply some filtering.</li>\n",
    "    <li> <b>analyzeAuthors : </b>Function to Analyze Authors & Reviews written by them.</li>\n",
    "    <li> <b>analyzeGenre : </b> Analyse Genre.</li>\n",
    "    <li> <b>analyzeCompanyGenre : </b>Analyze company & the genre of movies they produce.</li>\n",
    "    <li> <b>analyzeMovieTimelines:</b> Analyze movies released across years/month.</li>\n",
    "    <li> <b>analyzeCompanies: </b> Analyze movies produced across each production company</li>\n",
    "    <li> <b>analyzeLang: </b> Analyze movie languages</li>\n",
    "    <li> <b>analyzeRevenue: </b>Analyze Revenue </li>\n",
    "    <li> <b>analyzeVoteAvg: </b>Analyze Vote Average</li>\n",
    "</ul>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class TMDB\n",
    "class tmdb:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, limit, apikey, filename,start_year,end_year):\n",
    "        self.limit = limit\n",
    "        self.count = 0\n",
    "        # Can't fetch more than 1000 pages with the API\n",
    "        self.pagelimit = 1000\n",
    "        self.apikey = apikey\n",
    "        self.filename = filename\n",
    "        self.start = start_year\n",
    "        self.end = end_year\n",
    "        \n",
    "    # Function to check if request has reached limit in the last 10 seconds.    \n",
    "    def check_requestLimit(self):\n",
    "        # If Count is zero, start timer\n",
    "        if(self.count == 0):\n",
    "            self.start_time = time.time()\n",
    "        # Increase request count\n",
    "        self.count+=1\n",
    "        # Get Current time\n",
    "        current_time = time.time()\n",
    "        # For every 9 seconds limit requests\n",
    "        if(current_time - self.start_time <= 9):\n",
    "            # If has reached limit, sleep for 10 seconds & reset\n",
    "            if(self.count >= self.limit):\n",
    "                printmd(\"Going to sleep for 10 seconds since request threshold has reached\",color=\"red\")\n",
    "                time.sleep(10)\n",
    "                self.count = 0\n",
    "        else:\n",
    "            # At 10th seconds reset counter to start again\n",
    "            self.count = 0\n",
    "\n",
    "    # Fetch data set from API & Save to CSV               \n",
    "    def fetchFilmDetails(self):\n",
    "        # define column names for our new dataframe\n",
    "        columns = ['film','id','genres','overview','popularity','production_companies','original_title','original_language','release_date','revenue','vote_average','vote_count','reviews']\n",
    "        # create dataframe with columns\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        # For each year\n",
    "        for year in list(range(self.start,self.end+1)):\n",
    "            printmd(\"<b>Fetching for year:</b>\"+str(year),color=\"green\")\n",
    "            # Fetch Film Details\n",
    "            response = requests.get('https://api.themoviedb.org/3/discover/movie?api_key=' +  self.apikey+'&release_date.gte='+str(year)+'-01-01&release_date.lte='+str(year)+'-01-01')\n",
    "            tmdb.check_requestLimit()\n",
    "            films_json = response.json() # store parsed json response\n",
    "            # Get total number of pages for the results\n",
    "            pages = films_json['total_pages']\n",
    "            #loop at each page\n",
    "            for page in list(range(1, pages)):\n",
    "                # Get Response for current page\n",
    "                response = requests.get('https://api.themoviedb.org/3/discover/movie?api_key=' +  self.apikey +'&release_date.gte='+str(year)+'-01-01&release_date.lte='+str(year)+'-01-01&page='+str(page))\n",
    "                # Check if limit has reached\n",
    "                tmdb.check_requestLimit()\n",
    "                films_json = response.json() # store parsed json response\n",
    "                # Stop fetching for current year if page limit is hit\n",
    "                if(page > self.pagelimit):\n",
    "                    break\n",
    "                # Get results from json\n",
    "                films = films_json['results']\n",
    "                # for each of the film in the current page get details & Reviews\n",
    "                for film in films:\n",
    "                    # Get Film details\n",
    "                    film_details = requests.get('https://api.themoviedb.org/3/movie/'+ str(film['id']) +'?api_key='+ self.apikey +'&language=en-US')\n",
    "                    # Check if limit has reached\n",
    "                    tmdb.check_requestLimit()\n",
    "                    # Convert to json\n",
    "                    film_detail = film_details.json()\n",
    "                    # Get Reviews \n",
    "                    film_reviews = requests.get('http://api.themoviedb.org/3/movie/'+str(film['id'])+'/reviews?api_key='+self.apikey+'&language=en-US')\n",
    "                    # Check if limit has reached\n",
    "                    tmdb.check_requestLimit()\n",
    "                    # Convert to json\n",
    "                    film_review = film_reviews.json()\n",
    "                    # store all details in dataframe    \n",
    "                    df.loc[len(df)]=[film['title'],film_detail['id'],film_detail['genres'],film_detail['overview'],film_detail['popularity'],film_detail['production_companies'],film_detail['original_title'],film_detail['original_language'],film_detail['release_date'],film_detail['revenue'],film_detail['vote_average'],film_detail['vote_count'],film_review['results']]\n",
    "        # Save dataframe as csv\n",
    "        df.to_csv(self.filename)\n",
    "        printmd(\"Document Saved.\",color=\"green\")\n",
    "        printmd(\"<b>Number of records: </b>\"+df.shape[0],color=\"green\")\n",
    "    \n",
    "    # Function to Preprocess dataset & apply some filtering.\n",
    "    def processDetails(self):\n",
    "        \n",
    "        # Load CSV\n",
    "        self.films = pd.read_csv(self.filename, index_col=0)\n",
    "        printmd(\"**Cleaning & Filtering Data**\")\n",
    "        \n",
    "        # Convert date column to date time format\n",
    "        self.films['release_date'] = pd.to_datetime(self.films['release_date'])\n",
    "        \n",
    "        # Add seperate columns for month & year for analysis\n",
    "        printmd(\"<b>Columns before modification: </b>\"+ str(self.films.columns.tolist()),color=\"red\")\n",
    "        printmd(\"**Adding Columns month & year for analysis**\")\n",
    "        self.films['month'] = self.films['release_date'].dt.month\n",
    "        self.films['year'] = self.films['release_date'].dt.year\n",
    "        printmd(\"<b>Columns after modification: </b>\"+ str(self.films.columns.tolist()),color=\"green\")\n",
    "        \n",
    "        # Filter out films for the start & end date\n",
    "        years = list(self.films['year'].value_counts(sort=False))\n",
    "        years_ix = list(self.films['year'].value_counts(sort=False).index)\n",
    "        plotbar(zip(years_ix,years),\"Bar plot showing distribution of data over each year\",'Year','Count of Movies Released')  \n",
    "        printmd(\"**Filter dataset to reduce dataset to movies released between \"+str(self.start)+\" & \"+str(self.end)+\".**\")\n",
    "        self.films = self.films[(self.films['release_date'].dt.year > self.start) & (self.films['release_date'].dt.year < self.end)]\n",
    "        years = list(self.films['year'].value_counts(sort=False))\n",
    "        years_ix = list(self.films['year'].value_counts(sort=False).index)\n",
    "        plotbar(zip(years_ix,years),\"Bar plot showing distribution of data over each year after applying filter\",'Year','Count of Movies Released')\n",
    "        printmd(\"<b>Inference:</b> We can see that the TMDB database is not currently updated with recent years data but has a significant collection of movies from past 10 years.\",color=\"blue\")\n",
    "        \n",
    "        # Remove Duplicates\n",
    "        printmd(\"**Check for duplicate records**\")\n",
    "        duplicates = self.films[self.films.duplicated(subset =\"film\",keep=\"first\")]['film']\n",
    "        print(duplicates)\n",
    "        printmd(\"<b>Count of duplicate records: </b>\"+str(len(duplicates)),color=\"red\")\n",
    "        printmd(\"<b>Count of total records including duplicates: </b>\"+str(self.films.shape[0]),color=\"red\")\n",
    "        printmd(\"**Remove duplicate records**\")\n",
    "        self.films.drop_duplicates(subset =\"film\",keep = \"first\", inplace = True) \n",
    "        printmd(\"<b>Count of total records after removing duplicates: </b>\"+str(self.films.shape[0]),color=\"green\")\n",
    "        printmd(\"<b>Inference:</b> The TMDB database has duplicate records which are maintained. This could be added by different users.\",color=\"blue\")\n",
    "        \n",
    "        printmd(\"<b> Analyze genres in the dataset </b>\")\n",
    "        printmd(\"<b>ISSUE: </b> We can see that genres are surrounded with brackets & needs to split into a more readable form to be parse by python\",color=\"red\")\n",
    "        genres = self.films.iloc[10:20]['genres']\n",
    "        print(genres)\n",
    "        printmd(\"<b> Analyze production companies in the dataset </b>\")\n",
    "        printmd(\"<b>ISSUE: </b> We can see that companies are surrounded with brackets & needs to split into a more readable form to be parse by python\",color=\"red\")\n",
    "        companies = self.films.iloc[:10]['production_companies']\n",
    "        print(companies)\n",
    "        printmd(\"<b> Analyze reviews in the dataset </b>\")\n",
    "        printmd(\"<b>ISSUE: </b> We can see that reviews & authors are combined. Furthermore they are surrounded with brackets & needs to split into a more readable form to be parse by python\",color=\"red\")\n",
    "        reviews = self.films.iloc[-20:]['reviews']\n",
    "        print(reviews)\n",
    "        # For each film fetch Genres, Companies, Reviews & Authors\n",
    "        for index, row in self.films.iterrows():\n",
    "            # Fetch Genres and convert to list\n",
    "            genres_list = ast.literal_eval(row[2])\n",
    "            genres = list()\n",
    "            # For genres of the movie\n",
    "            for genre in genres_list:\n",
    "                # Append Genre to list\n",
    "                genres.append(genre['name'])\n",
    "            # Update Dataframe\n",
    "            self.films.loc[index,'genres'] = ','.join(genres)    \n",
    "            \n",
    "            # Fetch companies\n",
    "            prod_comp_list = ast.literal_eval(row[5])\n",
    "            prod_companies = list()\n",
    "            # Add each company of the movie\n",
    "            for company in prod_comp_list:\n",
    "                # Append company to list\n",
    "                prod_companies.append(company['name'])\n",
    "            # Update Dataframe\n",
    "            self.films.loc[index,'production_companies'] = ','.join(prod_companies)\n",
    "            \n",
    "            # Fetch Reviews\n",
    "            reviews_list = ast.literal_eval(row[12])\n",
    "            reviews = list()\n",
    "            authors = list()\n",
    "            # Add each review of the movie after removing punctuations & converting to lower case & ascii\n",
    "            for review in reviews_list:\n",
    "                stripped_review = re.sub(r\"[\\[\\]\\\"\\',-.;:@#?!&*$()/]+\\ *\", \" \", review['content'])\n",
    "                stripped_review = ''.join([t.lower() for t in stripped_review if t.isalnum() or ' '])\n",
    "                stripped_review = (stripped_review.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "                authors.append(review['author'])\n",
    "                reviews.append(stripped_review)\n",
    "                \n",
    "            # Update Dataframe\n",
    "            self.films.loc[index,'reviews'] = ','.join(reviews)\n",
    "            self.films.loc[index,'authors'] = ','.join(authors)\n",
    "        \n",
    "       \n",
    "        printmd(\"<b>Genres after processing</b>\",color=\"green\")\n",
    "        genres = self.films.iloc[10:20]['genres']\n",
    "        print(genres)\n",
    "        printmd(\"<b>Companies after processing</b>\",color=\"green\")\n",
    "        companies = self.films.iloc[:10]['production_companies']\n",
    "        print(companies)\n",
    "        printmd(\"<b>Reviews after processing generates two seperate columns for reviews & authors</b>\",color=\"green\")\n",
    "        printmd(\"<b>Reviews are also cleaned of special characters & converted to lower case</b>\",color=\"green\")\n",
    "        reviews = self.films.iloc[-10:]['reviews']\n",
    "        print(reviews)\n",
    "        authors = self.films.iloc[-10:]['authors']\n",
    "        print(authors)\n",
    "\n",
    "        printmd(\"<b>ISSUE: </b> Fields have missing entries due to data not being available\",color=\"red\")\n",
    "        printmd(\"<b>Replace empty fields with Nan</b>, will remove for each analysis based on fields\")\n",
    "        # Replace empty strings with Nan\n",
    "        self.films = self.films.replace(r'^\\s*$', np.nan, regex=True)       \n",
    "        printmd(\"<b>Reviews after processing</b>\",color=\"green\")\n",
    "        reviews = self.films.iloc[-10:]['reviews']\n",
    "        print(reviews)\n",
    "    \n",
    "    # Analyze Authors & Reviews written by them\n",
    "    def analyzeAuthors(self):\n",
    "        \n",
    "        printmd(\"<b> Dataset size before: </b>\"+str(self.films.shape[0]),color=\"red\")\n",
    "        printmd(\"<b> Remove Missing Entries </b>\")\n",
    "        # Get list of authors & Drop empty rows\n",
    "        films = self.films.dropna(subset=['authors'])\n",
    "        printmd(\"<b> Dataset size after: </b>\"+str(films.shape[0]),color=\"green\")\n",
    "        \n",
    "        author_list = list()\n",
    "        review_list = list()\n",
    "        \n",
    "        # Display authors with highest number of reviews\n",
    "        for index,film in films.iterrows():\n",
    "            film_authors = film['authors']\n",
    "            film_reviews = film['reviews']\n",
    "            author_list += film_authors.split(\",\")\n",
    "            review_list += film_reviews.split(\",\")\n",
    "        fd = nltk.FreqDist(author_list)\n",
    "        author_reviews = pd.DataFrame(np.column_stack([author_list, review_list]), \n",
    "                               columns=['authors', 'reviews'])\n",
    "        common_authors = fd.most_common(5)\n",
    "        printmd(\"<b> Show top 5 authors who have written lots of reviews </b> \")\n",
    "        plotbar(common_authors,\"Authors with highest number reviews\",'Author','Review count')\n",
    "        all_reviews = list()\n",
    "        authors_lexdiv = list()\n",
    "        authors = list()\n",
    "        printmd(\"<b>Inference:</b> Author \"+str(common_authors[0][0])+ \" has the most reviews (\"+str(common_authors[0][1])+\") written in the dataset.\",color=\"blue\")\n",
    "        printmd(\"<b>Inference:</b> Author \"+str(list(dict(fd.most_common()[-1:]).keys())[0])+ \" has the least reviews (\"+str(list(dict(fd.most_common()[-1:]).values())[0])+\") written in the dataset.\",color=\"blue\")\n",
    "        \n",
    "        # Analyse Reviews\n",
    "        reviews = review_list[0]\n",
    "        printmd(\"<b> Example of a review</b>\",color=\"red\")\n",
    "        print(reviews)\n",
    "        printmd(\"<b>Clean the sentence by removing stop words & stemming</b>\")\n",
    "        printmd(\"<b> Cleaned Review </b>\",color=\"green\")\n",
    "        printmd(' '.join(cleanText(reviews.split())))\n",
    "        \n",
    "        printmd(\"<b> Analysis of Reviews </b>\")\n",
    "        printmd(\"<b> Display common words used by each top author </b>\")\n",
    "        # For each author, analyze words he has used\n",
    "        for author in common_authors:\n",
    "            reviews = author_reviews.loc[author_reviews['authors'] == author[0]]['reviews']\n",
    "            review_total = list()\n",
    "            for review in reviews:\n",
    "                review_total += review.split()\n",
    "            cleaned_review =  cleanText(review_total) \n",
    "            all_reviews += cleaned_review\n",
    "            # Get frequency distribution\n",
    "            fd = nltk.FreqDist(cleaned_review)\n",
    "            # Display common 10 words used by top 5 authors\n",
    "            common_words = fd.most_common(10)\n",
    "            plotbar(common_words,\"Top 10 Common words used by \"+author[0],'Words','Count')\n",
    "            # Get lexical diversity of each of the authors\n",
    "            authors_lexdiv.append(lexical_diversity(cleaned_review))\n",
    "            authors.append(author[0])\n",
    "        # Plot lexical diversity\n",
    "        printmd(\"<b> Lexical Diversity </b>\")\n",
    "        printmd(\"The below plot signifies how well each author writes a review and whether he uses a creative vocabulary or repeats most of his critics among all his reviews\")\n",
    "        plotbar(zip(authors,authors_lexdiv),\"Lexical Diversity of top 5 Reviewers \",'Reviewers','Lexical Diversity')                                  \n",
    "        # Display wordcloud of common words\n",
    "        printmd(\"<b> Word Cloud </b>\")\n",
    "        printmd(\"The word cloud signifies common words used by the authors with bigger words signifying more usage among the reviews in the dataset.\")\n",
    "        disp_wordcloud(all_reviews, \"Common words used by top 5 reviewers\")\n",
    "\n",
    "    # Analyse Genre\n",
    "    def analyzeGenre(self):\n",
    "        \n",
    "        printmd(\"<b>Number of records: </b>\" + str(self.films.shape[0]),color=\"red\")\n",
    "        genres_list = list()\n",
    "        prod_genre = list()\n",
    "        printmd(\"<b>Remove movies with no genre</b>\")\n",
    "        # Drop empty rows\n",
    "        films = self.films.dropna(subset=['genres'])\n",
    "        printmd(\"<b>Number of records: </b>\" + str(films.shape[0]),color=\"green\")\n",
    "        \n",
    "        # For each film\n",
    "        for index,film in films.iterrows():\n",
    "            # Get genres for the film\n",
    "            film_genres = film['genres']\n",
    "            # For each genre in the film, create a list\n",
    "            genres_list += film_genres.split(\",\")\n",
    "            prod_genre +=  genres_list\n",
    "                \n",
    "        # Generate Frequency distribution of genres\n",
    "        fd = nltk.FreqDist(prod_genre)\n",
    "        genres = dict(fd)\n",
    "        plt.pie(genres.values(), labels=genres.keys(),autopct='%.2f', startangle=0)\n",
    "        plt.title(\"Distribution of genres across the data\")\n",
    "        plt.show()\n",
    "        printmd(\"<font color =blue><b> Inference: </b>The above pie chart shows Drama & Documentary movies are most captured in this database with both consisting of more than 16% each. Genres such as Adventure, Mystery are least captured in this dataset.</font>\")\n",
    "    \n",
    "    # Analyze company & the genre of movies they produce\n",
    "    def analyzeCompanyGenre(self):  \n",
    "        # Drop empty rows\n",
    "        printmd(\"<b>Number of records:</b>\" + str(self.films.shape[0]),color=\"red\")\n",
    "        company_list = list()\n",
    "        genres_list = list()\n",
    "        prod_genre = list()\n",
    "        printmd(\"<b>Remove movies with no genre or production company</b>\")\n",
    "        films = self.films.dropna(subset=['production_companies','genres'],how='any')\n",
    "        printmd(\"<b>Number of records:</b>\" + str(films.shape[0]),color=\"green\")\n",
    "        \n",
    "        # For each film, get companies & genres\n",
    "        for index,film in films.iterrows():\n",
    "            film_companies = film['production_companies']\n",
    "            film_genres = film['genres']\n",
    "            company_list += film_companies.split(\",\")\n",
    "            genres_list += film_genres.split(\",\")\n",
    "            # for each production company\n",
    "            for company in film_companies.split(\",\"):\n",
    "                # for each genre\n",
    "                prod_genre +=  genres_list\n",
    "                    \n",
    "        # Zip company+genre combo\n",
    "        prod_comp_genre = zip(company_list,prod_genre)\n",
    "        # Get frequency of the above combinations\n",
    "        fd = nltk.FreqDist(prod_comp_genre)\n",
    "        # Display top 30 companies\n",
    "        top_companies = dict(fd.most_common(30))\n",
    "        ser = pd.Series(list(top_companies.values()),\n",
    "                  index=pd.MultiIndex.from_tuples(top_companies.keys()))\n",
    "        df = ser.unstack().fillna(0)\n",
    "        sns.heatmap(df).set_title('Production Companies Vs. Genres - Most Number Of Movies(Top 30)')\n",
    "        # Display first heatmap\n",
    "        plt.show()\n",
    "        \n",
    "        # Display top 30 companies\n",
    "        least_companies = dict(fd.most_common(60)[-30:])\n",
    "        ser = pd.Series(list(least_companies.values()),\n",
    "                  index=pd.MultiIndex.from_tuples(least_companies.keys()))\n",
    "        df = ser.unstack().fillna(0)\n",
    "        sns.heatmap(df).set_title('Production Companies Vs. Genres - Most Number Of Movies(Top 30-60)')\n",
    "        top_companies = dict(fd.most_common())\n",
    "        printmd(\"<b>Inference:</b> Production/Genre\"+str(list(dict(fd.most_common()[:1]).keys())[0])+ \" has \"+str(list(dict(fd.most_common()[:1]).values())[0])+\" movie in the dataset.\",color=\"blue\")\n",
    "        printmd(\"<b>Inference:</b> Production/Genre\"+str(list(dict(fd.most_common()[-1:]).keys())[0])+ \" has \"+str(list(dict(fd.most_common()[-1:]).values())[0])+\" movie in the dataset.\",color=\"blue\")\n",
    "    \n",
    "    # Analyze movies released across years/month    \n",
    "    def analyzeMovieTimelines(self):\n",
    "        # Get list of movies & drop movies with no revenue or rating\n",
    "        printmd(\"<b> Movies per month </b>\")\n",
    "        months = list(self.films['month'].value_counts(sort=False))\n",
    "        months_ix = list(self.films['month'].value_counts(sort=False).index)\n",
    "        plotbar(zip(months_ix,months),\"Movies across the Months\",'Month','Count of Movies Released')\n",
    "        printmd(\"<b>Inference: </b>Movies are skewed towards January since the database has defaulted all movies without a release date to the the first day of the year of release\",color=\"blue\")\n",
    "        printmd(\"<b> Movies per year </b>\")\n",
    "        years = list(self.films['year'].value_counts(sort=False))\n",
    "        years_ix = list(self.films['year'].value_counts(sort=False).index)\n",
    "        plotbar(zip(years_ix,years),\"Movies across the Years\",'Year','Count of Movies Released')  \n",
    "        printmd(\"<b>Inference: </b>Movies in recent years hasn't been updated and have less data than past years\",color=\"blue\")\n",
    "        \n",
    "    # Analyze movies produced across each production company\n",
    "    def analyzeCompanies(self):\n",
    "        # Filter Data\n",
    "        printmd(\"<b>Number of records: </b>\" + str(self.films.shape[0]),color=\"red\")\n",
    "        # Get list of companies & Drop empty rows\n",
    "        films = self.films.dropna(subset=['production_companies'])\n",
    "        printmd(\"<b>Drop rows without production companies</b>\")\n",
    "        printmd(\"<b>Number of records: </b>\" + str(films.shape[0]),color=\"green\")\n",
    "        \n",
    "        company_list = list()\n",
    "        prod_year = list()\n",
    "        lang = list()\n",
    "        # For each film add list of years,language for each production company\n",
    "        for index,film in films.iterrows():\n",
    "            film_companies = film['production_companies']\n",
    "            company_list += film_companies.split(\",\")\n",
    "            for company in film_companies.split(\",\"):\n",
    "                prod_year += [film['year']]\n",
    "                lang += [film['original_language']]\n",
    "        # Combine company list, production year        \n",
    "        prod_comp_year = zip(company_list,prod_year)\n",
    "        prod_comp_lang = zip(company_list,lang)\n",
    "        \n",
    "        # Plot top 15 companies with high number of releases\n",
    "        fd = nltk.FreqDist(company_list)\n",
    "        top_companies = fd.most_common(15)\n",
    "        plotbar(top_companies,\"Top 15 Production Companies with high releases\",'Company','Movie count')\n",
    "        printmd(\"<b>Inference:</b> Company \"+str(list(dict(fd.most_common()[:1]).keys())[0])+ \" has highest released movies with \"+str(list(dict(fd.most_common()[:1]).values())[0])+\" movie in the dataset.\",color=\"blue\")\n",
    "        printmd(\"<b>Inference:</b> Company \"+str(list(dict(fd.most_common()[-1:]).keys())[0])+ \" has lowest released movies with \"+str(list(dict(fd.most_common()[-1:]).values())[0])+\" movie in the dataset.\",color=\"blue\")\n",
    "        \n",
    "        # Plot the top 60 movies each year released by these companies\n",
    "        # Plot top 30 first\n",
    "        fd = nltk.FreqDist( prod_comp_year)\n",
    "        top_companies = dict(fd.most_common(30))\n",
    "        ser = pd.Series(list(top_companies.values()),\n",
    "                  index=pd.MultiIndex.from_tuples(top_companies.keys()))\n",
    "        df = ser.unstack().fillna(0)\n",
    "        sns.heatmap(df).set_title('Production Companies Vs. Count of Movies Year - Top 30')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot 30-60\n",
    "        least_companies = dict(fd.most_common(60)[-30:])\n",
    "        ser = pd.Series(list(least_companies.values()),\n",
    "                  index=pd.MultiIndex.from_tuples(least_companies.keys()))\n",
    "        df = ser.unstack().fillna(0)\n",
    "        sns.heatmap(df).set_title('Production Companies Vs. Count of Movies Year - Top 30 to 60')\n",
    "        \n",
    "        # Plot the top 60 movies each year released by these companies\n",
    "        # Plot top 30 first\n",
    "        fd = nltk.FreqDist(prod_comp_lang)\n",
    "        top_companies = dict(fd.most_common(30))\n",
    "        ser = pd.Series(list(top_companies.values()),\n",
    "                  index=pd.MultiIndex.from_tuples(top_companies.keys()))\n",
    "        df = ser.unstack().fillna(0)\n",
    "        sns.heatmap(df).set_title('Production Companies Vs. Language - Top 30')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot 30-60\n",
    "        least_companies = dict(fd.most_common(60)[-30:])\n",
    "        ser = pd.Series(list(least_companies.values()),\n",
    "                  index=pd.MultiIndex.from_tuples(least_companies.keys()))\n",
    "        df = ser.unstack().fillna(0)\n",
    "        sns.heatmap(df).set_title('Production Companies Vs. Language - Top 30 to 60')\n",
    "\n",
    "    # Analyze languages\n",
    "    def analyzeLang(self):          \n",
    "     # For each film\n",
    "        lang = list()\n",
    "        for index,film in self.films.iterrows():\n",
    "            # Get languages for the film\n",
    "            film_lang = film['original_language']\n",
    "            # For each genre in the film, create a list\n",
    "            lang += [film_lang]\n",
    "        # Generate Frequency distribution of genres\n",
    "        fd = nltk.FreqDist(lang)\n",
    "        languages = dict(fd)\n",
    "        plt.pie(languages.values(), labels=languages.keys(),autopct='%.2f', startangle=0)\n",
    "        plt.title(\"Distribution of languages across the data\")\n",
    "        plt.show()\n",
    "        printmd(\"<font color =blue><b> Inference: </b>The above pie chart shows 75% of the movies are from the en(English) language</font>\")\n",
    "    \n",
    "    # Analyze Revenue\n",
    "    def analyzeRevenue(self):\n",
    "        x_axis_numbering = range(self.films['year'].min(), self.films['year'].max()+1)\n",
    "        plt.xticks(x_axis_numbering)\n",
    "        movies_year = (self.films[self.films.groupby(['year'])['revenue'].transform(max) == self.films['revenue']]).sort_values(by=['year'])\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Revenue')\n",
    "        plt.plot(movies_year['year'].values, movies_year['revenue'].values, label='Highest Revenue')\n",
    "        \n",
    "        # Remove zero revenue\n",
    "        films = self.films[self.films.revenue > 1000]\n",
    "        movies_year = (films[films.groupby(['year'])['revenue'].transform(min) == films['revenue']]).sort_values(by=['year'])\n",
    "        plt.title('Highest/Lowest Revenue Per Year')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Revenue')\n",
    "        plt.plot(movies_year['year'].values, movies_year['revenue'].values, label='Lowest Revenue')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Analyze Vote Average\n",
    "    def analyzeVoteAvg(self):\n",
    "        # Remove 10 average\n",
    "        films = self.films[self.films.vote_average != 10]\n",
    "        x_axis_numbering = range(films['year'].min(), films['year'].max()+1)\n",
    "        plt.xticks(x_axis_numbering)\n",
    "        movies_year = (films[films.groupby(['year'])['vote_average'].transform(max) == films['vote_average']]).sort_values(by=['year'])\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Vote Avg')\n",
    "        plt.plot(movies_year['year'].values, movies_year['vote_average'].values, label='Highest Vote Avg')\n",
    "        \n",
    "        # Remove zero average\n",
    "        films = self.films[self.films.vote_average != 0]\n",
    "        movies_year = (films[films.groupby(['year'])['vote_average'].transform(min) == films['vote_average']]).sort_values(by=['year'])\n",
    "        plt.title('Highest/Lowest Vote Average Per Year')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Vote Avg')\n",
    "        plt.plot(movies_year['year'].values, movies_year['vote_average'].values, label='Lowest Vote Avg')\n",
    "        plt.legend()\n",
    "        plt.show()        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Object for movies from 2000 to 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb = tmdb(REQUEST_LIMIT,APIKEY,FILENAME,2000,2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Data from API & Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.fetchFilmDetails()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Clean & Filter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.processDetails()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Authors of reviews\n",
    "\n",
    "Analyse Authors & Reviews in the dataset.\n",
    "<ul>\n",
    "    <li> Remove empty rows of authors & reviews. </li>\n",
    "    <li> Clean the sentence by removing stop words & stemming. </li>\n",
    "    <li> Fetch top 5 authors with highest number of reviews. </li>\n",
    "    <li> Display top 10 common words used in their reviews. </li>\n",
    "    <li> Show the lexical diversity of the reviews published by these authors.</li>\n",
    "    <li> Show the common words used in the reviews by these authors </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.analyzeAuthors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Genres\n",
    "\n",
    "Analyse Genres present in the dataset.\n",
    "\n",
    "<ul>\n",
    "    <li> Remove empty rows of genres. </li>\n",
    "    <li> Plot distribution of genres in the dataset. </li>\n",
    "    <li> Heatmap showing distribution of genres across different production companies. </li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.analyzeGenre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.analyzeCompanyGenre()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Movie Timelines\n",
    "\n",
    "Analyse Movies based on the release date in the dataset.\n",
    "\n",
    "<ul>\n",
    "    <li> Plot movies released each month. </li>\n",
    "    <li> Plot movies released each year. </li>\n",
    "</ul>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.analyzeMovieTimelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Production Companies\n",
    "\n",
    "Analyse Companies based on the movies they release.\n",
    "\n",
    "<ul>\n",
    "    <li> Plot movies released each year for each company. </li>\n",
    "    <li> Analyze movies released by language for each company. </li>\n",
    "</ul>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmdb.analyzeCompanies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Language\n",
    "\n",
    "Analyse languages which the movies are based on.\n",
    "\n",
    "<ul>\n",
    "    <li> Plot distribution of languages </li>\n",
    "</ul>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.analyzeLang()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Revenue & Vote Average\n",
    "\n",
    "Analyse revenue & vote average for each year.\n",
    "\n",
    "<ul>\n",
    "    <li> Plot highest revenue per year </li>\n",
    "    <li> Plot lowest revenue per year </li>\n",
    "    <li> Plot highest vote avg per year </li>\n",
    "    <li> Plot lowest vote avg per year </li>\n",
    "</ul>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.analyzeRevenue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.analyzeVoteAvg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentative Conclusion\n",
    "\n",
    "Further Analysis could be carried out alongside realtime data such as twitter:\n",
    "<ul>\n",
    "    <li> Comparison of tweets to the popularity & ratings </li>\n",
    "</ul>\n",
    "\n",
    "Following inferences were made from analysing the dataset:\n",
    "<ul>\n",
    "    <li> The highest revenue for a movie was made in 2009 </li>\n",
    "    <li> The dataset is skewed & incomplete with current real-time data </li>\n",
    "    <li> Release dates are incorrect or unavailable for many movies and have been defaulted to the first month of they year instead </li>\n",
    "    <li> Lots of movies are missing production company, genre etc </li>\n",
    "    <li> More reviews would give an in-depth analysis on the movies </li>\n",
    "<ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
